biplot(results_1, scale = 0)
biplot(results_2, scale = 0)
biplot(results_3, scale = 0)
# read and inspect dataframe
df_1 = read.csv('DPP1_all.csv')
df_2 = read.csv('DPP2_all.csv')
df_3 = read.csv("DPP3_all.csv")
# read and inspect dataframe
df_1 = read.csv('DPP1_all.csv')
df_2 = read.csv('DPP2_all.csv')
df_3 = read.csv("DPP3_all.csv")
#load libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(vegan)
library(ade4)
# remove rows with 0, else scaling wont work
df_1 = df_1[apply(df_1!=0, 1, all),]
df_2 = df_2[apply(df_2!=0, 1, all),]
df_3 = df_3[apply(df_3!=0, 1, all),]
# tranpose
df1_pca = t(df_1)
df2_pca = t(df_2)
df3_pca = t(df_3)
# make first row column names
colnames(df1_pca) = as.character(unlist(df1_pca[1, ]))
colnames(df2_pca) = as.character(unlist(df2_pca[1, ]))
colnames(df3_pca) = as.character(unlist(df3_pca[1, ]))
# replace previous column names and reset
df1_pca = df1_pca[-1, ]
rownames(df1_pca) = NULL
df2_pca = df2_pca[-1, ]
rownames(df2_pca) = NULL
df3_pca = df3_pca[-1, ]
rownames(df3_pca) = NULL
# calculate PCs
results_1 = prcomp(df1_pca, scale=TRUE)
results_2 = prcomp(df2_pca, scale = TRUE)
results_3 = prcomp(df3_pca, scale = TRUE)
# reverse eigenvectors
results_1$rotation = -1*results_1$rotation
results_2$rotation = -1*results_2$rotation
results_3$rotation = -1*results_3$rotation
# dispay PCs
results_1$rotation
results_2$rotation
results_3$rotation
# automatic biplot
par(mfrow = c(1, 3))
biplot(results_1, scale = 0)
biplot(results_2, scale = 0)
biplot(results_3, scale = 0)
# automatic biplot
par(mfrow = c(1, 3))
biplot(results_1, scale = 0, main = "DPP1")
biplot(results_2, scale = 0, main = "DPP2")
biplot(results_3, scale = 0, main = "DPP3")
# automatic biplot
par(mfrow = c(1, 3))
biplot(results_1, scale = 0)
title(main = "DPP1", line = 2.5)  # Adjust the value of 'line' to move the title
biplot(results_2, scale = 0)
title(main = "DPP2", line = 2.5)
biplot(results_3, scale = 0)
title(main = "DPP3", line = 2.5)
# perform procrustes
pro = procrustes(X = results_1,
Y = results_2,
symmetric = TRUE,
scale = TRUE)
# print the results
print(pro)
# perform procrustes
pro = procrustes(X = results_1,
Y = results_3,
symmetric = TRUE,
scale = TRUE)
# print the results
print(pro)
# perform procrustes
pro = procrustes(X = results_2,
Y = results_3,
symmetric = TRUE,
scale = TRUE)
# print the results
print(pro)
# take out the scores from first two principal components
scores_1 = results_1$x[, 1:2]
scores_2 = results_2$x[, 1:2]
scores_3 = results_3$x[, 1:2]
# check the dimensions - matrices must be the same dimensions
dim(scores_1)
dim(scores_2)
dim(scores_3)
# make distance matrix from derived scores
dist_1 = dist(scores_1)
dist_2 = dist(scores_2)
dist_3 = dist(scores_3)
# perform the Mantel test
mantel_result = mantel(dist_1,
dist_2,
method = "pearson",
permutations = 9999)
print(mantel_result)
# perform the Mantel test
mantel_result = mantel(dist_1,
dist_3,
method = "pearson",
permutations = 9999)
print(mantel_result)
# perform the Mantel test
mantel_result = mantel(dist_2,
dist_3,
method = "pearson",
permutations = 9999)
print(mantel_result)
# import libraries
library(dplyr)
library(pls)
setwd("C:/Users/Lenovo/Documents/Programiranje/PhD/SpectralTool/datasets/preprocessed spectra")
# import data
s1 = read.csv("DPP1_all.csv")
s2 = read.csv("DPP2_all.csv")
s3 = read.csv("DPP3_all.csv")
# lists of random columns for training and testing data
train_cols = c("Std_9",  # 1
"Std_7",  # 2
"Std_29", # 3
"Std_12", # 4
"Std_17", # 5
"Std_14", # 6
"Std_19", # 7
"Std_16", # 8
"Std_23", # 9
"Std_22", # 10
"Std_3",  # 11
"Std_26", # 12
"Std_1",  # 13
"Std_2",  # 14
"Std_28", # 15
"Std_27", # 16
"Std_15", # 17
"Std_24", # 18
"Std_10", # 19
"Std_21", # 20
"Std_18", # 21
"Std_5",  # 22
"Std_6",  # 23
"Std_30"  # 24
)
test_cols = c("Std_13", # 25
"Std_20", # 26
"Std_8",  # 27
"Std_11", # 28
"Std_4",  # 29
"Std_25"  # 30
)
# perform selection and save datasets
s1_train = s1 %>% select(all_of(train_cols))
s1_test = s1 %>% select(all_of(test_cols))
write.csv(s1_train, "DPP1_train.csv", row.names=FALSE)
write.csv(s1_test, "DPP1_test.csv", row.names=FALSE)
s2_train = s2 %>% select(all_of(train_cols))
s2_test = s2 %>% select(all_of(test_cols))
write.csv(s2_train, "DPP2_train.csv", row.names=FALSE)
write.csv(s2_test, "DPP2_test.csv", row.names=FALSE)
s3_train = s3 %>% select(all_of(train_cols))
s3_test = s3 %>% select(all_of(test_cols))
View(s3_train)
write.csv(s3_train, "DPP3_train.csv", row.names=FALSE)
write.csv(s3_test, "DPP3_test.csv", row.names=FALSE)
# import the data with dependant and independant variables
d1_train = read.csv("DPP1_train.csv")
d1_test = read.csv("DPP1_test.csv")
# train the model
model1 <- plsr(Yields ~ ., data = d1_train, validation = "CV")
summary(model1)
# validation and component number
validationplot(model1)
validationplot(model1, val.type="MSEP")
validationplot(model1, val.type="R2")
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=7)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=8)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=7)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm1 = lm(m1_pred ~ d1_test$Yields)
summary(lm1)
# plot pred vs actual
plot(d1_test$Yields, m1_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "Actual vs Predicted Values - OMNIC",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")  # Add a y=x line for reference
abline(lm1, col = "blue")
# plot pred vs actual
plot(d1_test$Yields, m1_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP1",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")  # Add a y=x line for reference
abline(lm1, col = "blue")
,#------#
d2_test = read.csv("DPP2_test.csv")
# train the model
model2 <- plsr(Yields ~ ., data = d2_train, validation = "CV")
summary(model2)
# validation and component number
validationplot(model2)
validationplot(model2, val.type="MSEP")
validationplot(model2, val.type="R2")
,#------#
# import the data with dependant and independant variables
d2_train = read.csv("DPP2_train.csv")
d2_test = read.csv("DPP2_test.csv")
# train the model
model2 <- plsr(Yields ~ ., data = d2_train, validation = "CV")
summary(model2)
# plot pred vs actual
plot(d2_test$Yields, m2_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "Actual vs Predicted Values - Spectragryph",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm2, col = "blue")
# validation and component number
validationplot(model2)
validationplot(model2, val.type="MSEP")
validationplot(model2, val.type="R2")
# validation and component number
par = (mfrow =c(1,3)
# validation and component number
par = (mfrow =c(1,3)
# validation and component number
par(mfrow =c(1,3))
validationplot(model2)
validationplot(model2, val.type="MSEP")
validationplot(model2, val.type="R2")
m2_pred <- predict(model2, d2_test, ncomp=8)
sqrt(mean((m2_pred - d2_test$Yields)^2))
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=8)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
# plot pred vs actual
plot(d2_test$Yields, m2_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "Actual vs Predicted Values - Spectragryph",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm2, col = "blue")
# import the data with dependant and independant variables
d3_train = read.csv("DPP3_train.csv")
d3_test = read.csv("DPP3_test.csv")
# train the model
model3 <- plsr(Yields ~ ., data = d3_train, validation = "CV")
summary(model3)
# validation and component number
par(mfrow =c(1,3))
validationplot(model3)
validationplot(model3, val.type="MSEP")
validationplot(model3, val.type="R2")
# make a predicion to training data
m3_pred <- predict(model3, d3_test, ncomp=8)
sqrt(mean((m3_pred - d3_test$Yields)^2))
# make a predicion to training data
m3_pred <- predict(model3, d3_test, ncomp=9)
sqrt(mean((m3_pred - d3_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm3 = lm(m3_pred ~ d3_test$Yields)
summary(lm3)
# make a predicion to training data
m3_pred <- predict(model3, d3_test, ncomp=8)
sqrt(mean((m3_pred - d3_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm3 = lm(m3_pred ~ d3_test$Yields)
summary(lm3)
# make a predicion to training data
m3_pred <- predict(model3, d3_test, ncomp=7)
sqrt(mean((m3_pred - d3_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm3 = lm(m3_pred ~ d3_test$Yields)
summary(lm3)
# make a predicion to training data
m3_pred <- predict(model3, d3_test, ncomp=8)
sqrt(mean((m3_pred - d3_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm3 = lm(m3_pred ~ d3_test$Yields)
summary(lm3)
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=9)
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=10)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=6)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=8)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
m2_pred <- predict(model2, d2_test, ncomp=7)
sqrt(mean((m2_pred - d2_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm2 = lm(m2_pred ~ d2_test$Yields)
summary(lm2)
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=8)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm1 = lm(m1_pred ~ d1_test$Yields)
summary(lm1)
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=9)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm1 = lm(m1_pred ~ d1_test$Yields)
summary(lm1)
# make a predicion to training data
m1_pred <- predict(model1, d1_test, ncomp=8)
sqrt(mean((m1_pred - d1_test$Yields)^2))
# Linear model of Pred. vs. Actual
lm1 = lm(m1_pred ~ d1_test$Yields)
summary(lm1)
# Print all graphs
par(mfrow =c(1,3))
plot(d1_test$Yields, m1_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP1",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")  # Add a y=x line for reference
abline(lm1, col = "blue")
plot(d2_test$Yields, m2_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP2",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm2, col = "blue")
plot(d3_test$Yields, m3_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP3",
xlim = c(0, 25),
ylim = c(0, 25),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm3, col = "blue")
# Print all graphs
par(mfrow =c(1,3))
plot(d1_test$Yields, m1_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP1",
xlim = c(0, 30),
ylim = c(0, 30),
col = "black",
pch = 16
)
abline(0, 1, col = "red")  # Add a y=x line for reference
abline(lm1, col = "blue")
plot(d2_test$Yields, m2_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP2",
xlim = c(0, 30),
ylim = c(0, 30),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm2, col = "blue")
plot(d3_test$Yields, m3_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP3",
xlim = c(0, 30),
ylim = c(0, 30),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm3, col = "blue")
# Print all graphs
par(mfrow =c(1,3))
plot(d1_test$Yields, m1_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP1",
xlim = c(0, 35),
ylim = c(0, 35),
col = "black",
pch = 16
)
abline(0, 1, col = "red")  # Add a y=x line for reference
abline(lm1, col = "blue")
plot(d2_test$Yields, m2_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP2",
xlim = c(0, 35),
ylim = c(0, 35),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm2, col = "blue")
plot(d3_test$Yields, m3_pred,
xlab = "Actual Values",
ylab = "Predicted Values",
main = "DPP3",
xlim = c(0, 35),
ylim = c(0, 35),
col = "black",
pch = 16
)
abline(0, 1, col = "red")
abline(lm3, col = "blue")
validationplot(model1, main = "DPPH1")
par(mfrow =c(1,3))
validationplot(model1, main = "DPPH1")
validationplot(model2, main = "DPPH2")
validationplot(model3, main = "DPPH3")
par(mfrow =c(1,3))
validationplot(model1, main = "DPP1")
validationplot(model2, main = "DPP2")
validationplot(model3, main = "DPP3")
# import libraries
library(dplyr)
library(pls)
par(mfrow =c(1,3))
validationplot(model1, main = "DPP1")
validationplot(model2, main = "DPP2")
validationplot(model3, main = "DPP3")
# load libraries
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(tidyr)
install.packages("ggplot2")
# load libraries
library(ggplot2)
R.version
R.version
